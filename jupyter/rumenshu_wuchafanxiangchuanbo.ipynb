{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "81d54f8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#误差反向传播法：高效计算权重参数的梯度方法\n",
    "#复合函数的导数，链式法则，是一种累乘\n",
    "#加法节点：z=x+y （加法的反向传播：上一级值等于本级值）\n",
    "#乘法节点：z=x*y（乘法的反向传播：上一级输入信号*本级翻转值）\n",
    "\n",
    "#简单层的实现\n",
    "#乘法层：计算图的乘法节点\n",
    "#加法层：计算图的加法节点\n",
    "\n",
    "#层：神经网络中功能的单位。神经网络中的层实现为一个类。负责sigmoid函数的sigmoid，以层来实现\n",
    "#层的实现有两个共通的方法（接口）：forward()//正向传播 backward()//反向传播"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e4535253",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "220.00000000000003\n",
      "1.1\n",
      "2.2 110.00000000000001 200\n"
     ]
    }
   ],
   "source": [
    "#乘法层\n",
    "class MulLayer:\n",
    "    def __init__(self):#初始化实例变量x与y，保存正向传播时的输入值\n",
    "        self.x=None\n",
    "        self.y=None\n",
    "    def forward(self,x,y):\n",
    "        self.x=x\n",
    "        self.y=y\n",
    "        out=x*y\n",
    "        return out\n",
    "    def backward(self,dout):#从上游传来的导数（dout）乘以正向传播的翻转值，传给下游\n",
    "        dx=dout*self.y#翻转x和y\n",
    "        dy=dout*self.x\n",
    "        return dx,dy\n",
    "    \n",
    "apple=100\n",
    "apple_num=2\n",
    "tax=1.1\n",
    "#layer\n",
    "mul_apple_layer=MulLayer()\n",
    "mul_tax_layer=MulLayer()\n",
    "#forward\n",
    "apple_price=mul_apple_layer.forward(apple,apple_num)\n",
    "price=mul_tax_layer.forward(apple_price,tax)\n",
    "print(price)#220.00000000000003\n",
    "#backward\n",
    "dprice=1\n",
    "dapple_price,dtax=mul_tax_layer.backward(dprice)\n",
    "print(dapple_price)#1.1\n",
    "dapple,dapple_num=mul_apple_layer.backward(dapple_price)\n",
    "print(dapple,dapple_num,dtax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "310e3ad6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#加法层\n",
    "class AddLayer:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    def forward(self,x,y):\n",
    "        out=x+y\n",
    "        return out\n",
    "    def backward(self,dout):\n",
    "        dx=dout*1\n",
    "        dy=dout*1\n",
    "        return dx,dy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4f0e0f7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#实现2个苹果和3个橘子计算图\n",
    "apple=100\n",
    "apple_num=2\n",
    "orange=150\n",
    "orange_num=3\n",
    "tax=1.1\n",
    "#layer\n",
    "mul_apple_layer=MulLayer()\n",
    "mul_orange_layer=MulLayer()\n",
    "add_apple_orange_layer=AddLayer()\n",
    "mul_tax_layer=MulLayer()\n",
    "#forward\n",
    "apple_price=mul_apple_layer.forward(apple,apple_num)\n",
    "orange_price=mul_orange_layer.forward(orange,orange_num)\n",
    "all_price=add_apple_orange_layer.forward(apple_price,orange_price)\n",
    "price=mul_tax_layer.forward(all_price,tax)\n",
    "#backward\n",
    "dprice=1\n",
    "dall_price,dtax=mul_tax_layer.backward(dprice)#其实发现参数上下对应，就是函数顺序颠倒了\n",
    "dapple_price,dorange_price=add_apple_orange_layer.backward(dall_price)\n",
    "dorange,dorange_num=mul_orange_layer.backward(dorange_price)\n",
    "dapple,dapple_num=mul_apple_layer.backward(dapple_price)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "093309c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "715.0000000000001\n"
     ]
    }
   ],
   "source": [
    "print(price)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a7f37261",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "110.00000000000001 2.2 3.3000000000000003 165.0 650\n"
     ]
    }
   ],
   "source": [
    "print(dapple_num,dapple,dorange,dorange_num,dtax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "83a83c5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#将计算图的思路应用到神经网络中\n",
    "#ReLu层（正向传播时输入x大于0，，正向传播值为本身，反向传播时（求导）将上游值原封不动传给下游；x小于等于0，反向传播时值为0，上游信号停在此处）\n",
    "#forward 与backward是numpy数组\n",
    "class Relu:\n",
    "    def __init__(self):\n",
    "        self.mask=None\n",
    "    def forward(self,x):\n",
    "        self.mask=(x<=0)#mask数组是bool型\n",
    "        out=x.copy()\n",
    "        out[self.mask]=0\n",
    "        return out\n",
    "    def backward(self,dout):\n",
    "        dout[self.mask]=0\n",
    "        dx=dout\n",
    "        return dx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3b2d20c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.  -0.5]\n",
      " [-2.   3. ]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "x=np.array([[1.0,-0.5],[-2.0,3.0]])\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8cf17aa1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[False  True]\n",
      " [ True False]]\n"
     ]
    }
   ],
   "source": [
    "mask=(x<=0)\n",
    "print(mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "122c9e49",
   "metadata": {},
   "outputs": [],
   "source": [
    "#sigmoid层\n",
    "#多了“/节点”和“exp”节点\n",
    "#但简化后，只和正向传播的输出和上游值有关\n",
    "class sigmoid:\n",
    "    def __init__(self):\n",
    "        self.out=None\n",
    "    def forward(self,x):\n",
    "        out=1/(1+np.exp(-x))\n",
    "        self.out=out\n",
    "        return out\n",
    "    def backward(self,dout):\n",
    "        dx=dout*self.out*(1.0-self.out)\n",
    "        return dx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3dce3822",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Affine/Softmax层:np.dot(x,w)+b的正向反向传播\n",
    "class Affine:\n",
    "    def __init__(self,W,b):\n",
    "        self.W=W#需要不同函数之间使用时，才self.xx创建变量，当变量需要外部输入时，才传入函数参数\n",
    "        self.b=b\n",
    "        self.x=None\n",
    "        self.dW=None\n",
    "        self.db=None\n",
    "    def forward(self,x):\n",
    "        self.x=x\n",
    "        out=np,dot(x,self.W)+self.b\n",
    "        return out\n",
    "    def backward(self,dout):\n",
    "        dx=np.dot(dout,self.W.T)\n",
    "        self.dW=np.dot(self.x.T,dout)\n",
    "        self.db=np.sum(dout,axis=0)#多批量输入数据\n",
    "        return dx\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8566711f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#输入图像通过Affine层和ReLu层进行转换（一层Affine一层ReLu一层Affine一层ReLu...最后一层Affine），最后一层Affine再通过softmax层进行正规化\n",
    "#最后一层Affine输出10个数据，数值为得分，得分高的所在位置为对应类别。softmax输入为将这10个数据，输出为10个数据，输出是概率值（0<值<1）\n",
    "#10分类因为这个例子是数字识别\n",
    "\n",
    "#神经网络中进行的处理有推理和学习。\n",
    "#推理时,只需要给出答案，所以只对得分最大值感兴趣，会将最后一个Affine层的输出作为识别结果（神经网络中未被正规化的输出结果称为“得分”）\n",
    "#学习时，需要softmax层\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "43d21947",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Softmax-with-Loss层\n",
    "#包含softmax层以及作为损失函数的交叉熵误差\n",
    "\n",
    "#正向传播：假设要进行3分类，从前面的层接收3个输入（得分）。softmax层将输入（a1,a2,a3）正规化，输出（y1,y2,y3）。\n",
    "#cross entropy error层接收softmax的输出（y1,y2,y3）和监督标签(t1,t2,t3),从这些数据中输出损失L\n",
    "\n",
    "#（y1,y2,y3）是softmax层的输出\n",
    "#（t1,t2,t3）是监督数据\n",
    "#softmax层的反向传播输出得到了（y1-t1,y2-t2,y3-t3）的结果，是softmax层的输出与监督数据的差分\n",
    "#神经网络的反向传播会把这个差分表示的误差传递给前面的层，这是神经网络学习的重要性质\n",
    "\n",
    "#神经网络学习的目的就是通过调整权重参数，使神经网络的输出接近监督标签\n",
    "#因此必须将神经网络的输出与监督标签的误差高效的传给前面的层：这里就是差分\n",
    "#举例：监督标签（0,1,0），softmax层输出（0.3,0.2，0.5），正确解标签处的概率是0.2（20%），这时候神经网络未能正确识别。softmax层反向传递\n",
    "#（0.3,-0.8,0.5）这样一个大的误差，大的误差向前面的层传播，神经网络会学习到“大”的内容\n",
    "\n",
    "#为何Softmax-with-Loss层反向传播结果是（y1-t1,y2-t2,y3-t3）\n",
    "#因为：使用交叉熵误差函数作为softmax函数的损失函数\n",
    "#使用平方和误差函数作为恒等函数（输入等于输出）的误差函数，也可以得到（y1-t1,y2-t2,y3-t3）\n",
    "\n",
    "class SoftmaxWithLoss:\n",
    "    def __init__(self):\n",
    "        self.loss=None#损失\n",
    "        self.y=None#损失函数的输出\n",
    "        self.t=None#监督数据one-hot vector\n",
    "    def forward(self,x,t):\n",
    "        self.t=t\n",
    "        self.y=softmax(x)\n",
    "        self.loss=cross_entropy_error(self.y,self.t)\n",
    "        return self.loss\n",
    "    def backward(self,dout=1):\n",
    "        batch_size=self.t.shape[0]\n",
    "        dx=(self.y-self.t)/batch_size#除以批的大小，传给前面的层是单个数据的误差\n",
    "        return dx\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "f83a98a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys,os\n",
    "sys.path.append(os.getcwd())\n",
    "import numpy as np\n",
    "from common.layers import *\n",
    "from common.gradient import numerical_gradient\n",
    "from collections import OrderedDict#y有序字典，会记住向字典里添加的元素的顺序\n",
    "class TwoLayerNet:\n",
    "    def __init__(self,input_size,hidden_size,putput_size,weight_init_std=0.01):\n",
    "        #初始化权重\n",
    "        self.params={}\n",
    "        self.params[\"W1\"]=weight_init_std*np.random.randn(input_size,hidden_size)#索引为“W1”的是数组\n",
    "        self.params[\"b1\"]=np.zeros(hidden_size)\n",
    "        self.params[\"W2\"]=weight_init_std*np.random.randn(hidden_size,output_size)\n",
    "        self.params[\"b2\"]=np.zeros(output_size)\n",
    "        #生成层(叠乐高一样一层一层往上叠)\n",
    "        self.layers=OrderedDict()\n",
    "        self.layers[\"Affine1\"]=Affine(self.params[\"W1\"],self.params[\"b1\"])\n",
    "        self.layers[\"Relu1\"]=Relu()\n",
    "        self.layers[\"Affine2\"]=Affine(self.params[\"W2\"],self.params[\"b2\"])\n",
    "        self.lastlayer=SoftmaxWithLoss()\n",
    "    def predict(self,x):\n",
    "        for layer in self.layers.values():#self.layers.values()，组成元素是层，key是索引“Affine”\n",
    "            x=layer.forward(x)\n",
    "        return x\n",
    "    def loss(self,x,t):\n",
    "        y=self.predict(x)\n",
    "        return self.lastlayer.forward(y,t)\n",
    "    def accuracy(self,x,t):\n",
    "        y=self.predict(x)\n",
    "        y=np.argmax(y,axis=1)\n",
    "        if t.ndim!=1:\n",
    "            t=np.argmax(t,axis=1)#one-hot\n",
    "        accuracy=np.sum(y==t)/float(x.shape[0])\n",
    "        return accuracy\n",
    "    def numerical_gradient(self,x,t):#数值微分求梯度\n",
    "        loss_W=lambda W:self.loss(x,t)\n",
    "        grads={}\n",
    "        grads[\"W1\"]=numerical_gradient(loss_W,self.params[\"W1\"])#函数名一样但是参数个数不一样\n",
    "        grads[\"b1\"]=numerical_gradient(loss_W,self.params[\"b1\"])\n",
    "        grads[\"W2\"]=numerical_gradient(loss_W,self.params[\"W2\"])\n",
    "        grads[\"b2\"]=numerical_gradient(loss_W,self.params[\"b2\"])\n",
    "        return grads\n",
    "    def gradient(self,x,t):#反向传输求梯度\n",
    "        #forward\n",
    "        self.loss(x,t)\n",
    "        #backward\n",
    "        dout=1\n",
    "        dout=self.lastLayer.backward(dout)\n",
    "        layers=list(self.layers.values())#list1 = ['physics', 'chemistry', 1997, 2000] print \"list1[0]: \", list1[0] //list1[0]:  physics\n",
    "        layers.reverse()##list.reverse()，对列表元素进行反向排列\n",
    "        for layer in layers:\n",
    "            dout=layer.backward(dout)\n",
    "        #设定\n",
    "        grads={}\n",
    "        grads[\"W1\"]=self.layers[\"Affines1\"].dW\n",
    "        grads[\"b1\"]=self.layers[\"Affines1\"].db\n",
    "        grads[\"W2\"]=self.layers[\"Affines2\"].dW\n",
    "        grads[\"b2\"]=self.layers[\"Affines2\"].db\n",
    "        return grads\n",
    "    \n",
    "  \n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c3e9afc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "[1 2]\n",
      "1\n",
      "[3 4]\n"
     ]
    }
   ],
   "source": [
    "x=np.array([[1,2],[3,4]])\n",
    "for idx, x in enumerate(x):\n",
    "    print(idx)\n",
    "    print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "7d028de5",
   "metadata": {},
   "outputs": [],
   "source": [
    " #self.layers.values()  //layers的元素\n",
    "#dishes = {'eggs': 2, 'sausage': 1, 'bacon': 1, 'spam': 500}\n",
    "#keys = dishes.keys()\n",
    "#values = dishes.values()\n",
    "#>>> # 迭代\n",
    "#>>> n = 0\n",
    "#>>> for val in values:\n",
    "#...     n += val\n",
    "#>>> print(n)\n",
    "#504\n",
    "\n",
    "#>>> # keys 和 values 以相同顺序（插入顺序）进行迭代\n",
    "#>>> list(keys)     # 使用 list() 转换为列表\n",
    "#['eggs', 'sausage', 'bacon', 'spam']\n",
    "#>>> list(values)\n",
    "#[2, 1, 1, 500]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "20e75ede",
   "metadata": {},
   "outputs": [],
   "source": [
    "#以上实现代码：\n",
    "#将神经网络的层保存为OrderedDict这一点非常重要。OrderDict是有序字典，“有序”是指它可以记住向字典里添加元素的顺序\n",
    "#神经网络正向传播只需按照添加元素的顺序，调用各层的forward()方法就可以完成处理。而反向传播只需要按照相反的顺序调用各层。\n",
    "\n",
    "#因为Affine层和ReLU层的内部会正确处理正向传播和反向传播，所以这里仅仅是以正确的顺序连接各层，再按顺序（或者逆序）调用各层\n",
    "\n",
    "#像这样通过将神经网络的组成元素以层的方式实现，可以轻松的构建神经网络。\n",
    "#用层进行模块化的实现具有很大优势。因为如果想另外构建一个神经网络（N层），只需要像组装乐高积木那样添加必要层就可以了。之后，\n",
    "#通过各层内部实现的正向传播和反向传播，就可正确计算进行识别以及所需梯度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "f856c1e5",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n",
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n",
      "W1:2.162681404935406e-10\n",
      "[ 1.58157529e-03  2.08517996e-03  3.91271147e-04 -1.73121549e-04\n",
      "  5.01897002e-04  2.68009589e-04  2.63441547e-04  6.00789606e-04\n",
      "  1.78196722e-04  1.91918033e-03 -7.99633878e-04  2.66332198e-04\n",
      " -3.28289378e-04  4.27201522e-05  2.37272700e-04  1.81698056e-03\n",
      "  2.52448820e-04 -8.97545318e-04 -4.21030426e-04 -4.71887234e-04\n",
      " -7.53862402e-04 -9.03932395e-05 -1.30732735e-03 -1.04167895e-03\n",
      "  1.51972141e-03  2.59372005e-03 -1.15084497e-04  1.33178448e-03\n",
      "  1.80002035e-03 -1.48027231e-05  9.78016878e-04  3.43179233e-03\n",
      "  1.10263052e-03  1.18863214e-03 -6.50195491e-04 -4.74212040e-04\n",
      "  6.15965442e-04 -4.18172012e-04  1.48891297e-04 -2.74121709e-06\n",
      "  1.01068221e-03 -1.37910672e-03 -8.66784051e-04 -3.02918868e-03\n",
      "  1.35723647e-03  4.87140336e-06  2.26969916e-03 -2.74939443e-03\n",
      " -2.52601933e-04  2.89267185e-03]\n",
      "[ 1.58157367e-03  2.08517787e-03  3.91270694e-04 -1.73121357e-04\n",
      "  5.01896438e-04  2.68009341e-04  2.63441278e-04  6.00788996e-04\n",
      "  1.78196518e-04  1.91917839e-03 -7.99633020e-04  2.66331937e-04\n",
      " -3.28288994e-04  4.27200009e-05  2.37272439e-04  1.81697865e-03\n",
      "  2.52448444e-04 -8.97544368e-04 -4.21029989e-04 -4.71886772e-04\n",
      " -7.53861602e-04 -9.03931885e-05 -1.30732606e-03 -1.04167785e-03\n",
      "  1.51971983e-03  2.59371736e-03 -1.15084422e-04  1.33178312e-03\n",
      "  1.80001854e-03 -1.48026325e-05  9.78015828e-04  3.43178874e-03\n",
      "  1.10262937e-03  1.18863092e-03 -6.50194860e-04 -4.74211594e-04\n",
      "  6.15964775e-04 -4.18171560e-04  1.48891146e-04 -2.74130052e-06\n",
      "  1.01068117e-03 -1.37910526e-03 -8.66783161e-04 -3.02918555e-03\n",
      "  1.35723509e-03  4.87130780e-06  2.26969685e-03 -2.74939163e-03\n",
      " -2.52601602e-04  2.89266891e-03]\n",
      "b1:1.0186562108459524e-09\n",
      "[[-0.10191958  0.04910065  0.04541028  0.05027489 -0.11469248 -0.11681534\n",
      "   0.04855717  0.04707135  0.04674296  0.0462701 ]\n",
      " [-0.11483577  0.05278232  0.04881386  0.05404362 -0.11769836 -0.12589137\n",
      "   0.05219703  0.05060176  0.05024751  0.0497394 ]\n",
      " [-0.12042897  0.05287646  0.04889993  0.05413945 -0.11427445 -0.12436108\n",
      "   0.05228988  0.05069304  0.05033715  0.04982859]\n",
      " [-0.11880294  0.05247451  0.04852932  0.05372899 -0.11863015 -0.11890832\n",
      "   0.05189434  0.05030778  0.04995532  0.04945115]\n",
      " [-0.11599507  0.05367093  0.04963605  0.05495381 -0.12144474 -0.1270216\n",
      "   0.05307632  0.05145357  0.05109366  0.05057707]\n",
      " [-0.11726255  0.05175249  0.04786165  0.05298977 -0.1172943  -0.11688198\n",
      "   0.05118044  0.04961561  0.04926803  0.04877085]\n",
      " [-0.12140496  0.05332152  0.04931188  0.05459551 -0.11693218 -0.12375188\n",
      "   0.05273068  0.0511198   0.05076114  0.05024849]\n",
      " [-0.11983105  0.0527005   0.04873674  0.0539588  -0.11157461 -0.12646072\n",
      "   0.0521149   0.05052419  0.05016923  0.04966203]\n",
      " [-0.11337866  0.05076384  0.04694901  0.05197908 -0.12287778 -0.10847777\n",
      "   0.0502055   0.04866779  0.04832799  0.047841  ]\n",
      " [-0.12570591  0.0530128   0.04902506  0.05427848 -0.11091124 -0.12337258\n",
      "   0.05242436  0.05082469  0.05046692  0.04995743]\n",
      " [-0.12848528  0.05472306  0.05060864  0.05603149 -0.12379333 -0.11933616\n",
      "   0.0541191   0.05246451  0.05209649  0.05157149]\n",
      " [-0.11465692  0.05227706  0.04834808  0.0535279  -0.12400295 -0.11634534\n",
      "   0.05170058  0.05011794  0.04976791  0.04926573]\n",
      " [-0.11222234  0.05044089  0.04664891  0.05164691 -0.1151283  -0.11518075\n",
      "   0.0498832   0.04835775  0.04801929  0.04753445]\n",
      " [-0.10632075  0.05189295  0.04799411  0.05313527 -0.12765897 -0.11841676\n",
      "   0.05132085  0.04974824  0.04940212  0.04890294]\n",
      " [-0.12891943  0.05460015  0.05049432  0.05590503 -0.120472   -0.12138528\n",
      "   0.05399649  0.05234669  0.05197905  0.05145498]\n",
      " [-0.11422156  0.05216711  0.04824471  0.05341358 -0.11553632 -0.12448954\n",
      "   0.05158849  0.0500121   0.0496618   0.04915963]\n",
      " [-0.10950655  0.04985212  0.04610559  0.05104516 -0.11925748 -0.10977529\n",
      "   0.04930282  0.04779324  0.04745956  0.04698082]\n",
      " [-0.10926968  0.0511093   0.04726855  0.05233244 -0.12290499 -0.11490026\n",
      "   0.05054572  0.04899786  0.04865614  0.04816491]\n",
      " [-0.12269702  0.05460533  0.05049887  0.05590957 -0.11807438 -0.13003188\n",
      "   0.05399922  0.05235015  0.05198281  0.05145734]\n",
      " [-0.10545875  0.04924874  0.04554801  0.05042758 -0.11943638 -0.10954654\n",
      "   0.04870613  0.04721425  0.04688508  0.04641189]\n",
      " [-0.11592827  0.05266053  0.04870228  0.05392012 -0.12282345 -0.11885516\n",
      "   0.05207908  0.05048557  0.05013267  0.04962662]\n",
      " [-0.11430142  0.05163078  0.0477492   0.05286504 -0.11698693 -0.11932165\n",
      "   0.05105946  0.04949833  0.04915183  0.04865536]\n",
      " [-0.11467896  0.05334179  0.04933099  0.05461607 -0.11733603 -0.13020633\n",
      "   0.05274933  0.05113774  0.05077967  0.05026574]\n",
      " [-0.11589819  0.05199395  0.04808645  0.05323838 -0.12469262 -0.11249527\n",
      "   0.05142156  0.049847    0.04949885  0.04899987]\n",
      " [-0.10586934  0.04886245  0.04519053  0.05003192 -0.11773117 -0.10821792\n",
      "   0.04832406  0.04684415  0.04651735  0.04604797]\n",
      " [-0.11807327  0.05230599  0.04837292  0.05355584 -0.11552251 -0.12159729\n",
      "   0.0517265   0.05014601  0.04979438  0.04929143]\n",
      " [-0.11067746  0.0514374   0.04757163  0.05266808 -0.122079   -0.11654498\n",
      "   0.05086972  0.04931248  0.0489683   0.04847384]\n",
      " [-0.11875318  0.05330757  0.04929927  0.0545813  -0.11777458 -0.12546544\n",
      "   0.05271664  0.05110588  0.05074769  0.05023484]\n",
      " [-0.11013808  0.0498078   0.04606366  0.05099887 -0.11474877 -0.11334619\n",
      "   0.04925738  0.0477507   0.04741669  0.04693796]\n",
      " [-0.12163252  0.05390736  0.04985552  0.05519716 -0.12700291 -0.11744261\n",
      "   0.05331332  0.05168162  0.05132022  0.05080284]\n",
      " [-0.10628231  0.05071206  0.04690091  0.05192525 -0.12024574 -0.1178452\n",
      "   0.0501517   0.04861647  0.04827742  0.04778944]\n",
      " [-0.11108116  0.05229042  0.04836056  0.0535414  -0.12387464 -0.12013681\n",
      "   0.0517129   0.05012993  0.04978016  0.04927723]\n",
      " [-0.12123952  0.05352783  0.0495039   0.05480795 -0.12317996 -0.11907774\n",
      "   0.0529369   0.05131773  0.05095846  0.05044444]\n",
      " [-0.11329782  0.05267404  0.04871279  0.0539317  -0.11270169 -0.13168321\n",
      "   0.05208773  0.05049744  0.05014345  0.04963558]\n",
      " [-0.10490654  0.05028055  0.04650215  0.05148371 -0.12067099 -0.11586698\n",
      "   0.04972543  0.04820276  0.04786682  0.04738309]\n",
      " [-0.11617576  0.05110733  0.04726508  0.05232931 -0.11635925 -0.11452371\n",
      "   0.05054271  0.04899719  0.04865398  0.04816312]\n",
      " [-0.11155479  0.05161431  0.04773284  0.05284679 -0.11093509 -0.1279979\n",
      "   0.05104012  0.04948164  0.04913479  0.04863728]\n",
      " [-0.12052437  0.05179089  0.04789695  0.05302914 -0.11705315 -0.11412433\n",
      "   0.051219    0.04965311  0.04930491  0.04880785]\n",
      " [-0.11465712  0.0508608   0.04703652  0.05207623 -0.11282756 -0.11789516\n",
      "   0.05029749  0.04876049  0.04841866  0.04792964]\n",
      " [-0.13446799  0.0556053   0.05142173  0.05693225 -0.11294384 -0.13018025\n",
      "   0.05498731  0.0533106   0.05293465  0.05240025]\n",
      " [-0.10564894  0.05072889  0.04691729  0.05194327 -0.12412561 -0.11471753\n",
      "   0.05016973  0.04863264  0.04829402  0.04780624]\n",
      " [-0.10939368  0.05093827  0.04710932  0.05215628 -0.117485   -0.11902857\n",
      "   0.05037468  0.04883377  0.04849252  0.0480024 ]\n",
      " [-0.11501808  0.05227272  0.04834265  0.05352204 -0.11719359 -0.1227553\n",
      "   0.05169363  0.05011354  0.04976265  0.04925974]\n",
      " [-0.1205625   0.05235863  0.04842146  0.05360979 -0.11542018 -0.11956961\n",
      "   0.05177901  0.05019699  0.04984474  0.04934166]\n",
      " [-0.11809562  0.05227377  0.04834386  0.05352361 -0.11901361 -0.11786991\n",
      "   0.0516961   0.05011531  0.04976434  0.04926215]\n",
      " [-0.11817547  0.05183415  0.04793629  0.05307254 -0.11339198 -0.12042105\n",
      "   0.05125971  0.04969386  0.04934515  0.0488468 ]\n",
      " [-0.11583092  0.05223466  0.04830709  0.05348277 -0.1155821  -0.12329418\n",
      "   0.05165558  0.05007717  0.04972626  0.04922368]\n",
      " [-0.11751973  0.05198445  0.04807636  0.05322745 -0.11869825 -0.11679695\n",
      "   0.05141013  0.04983797  0.04948897  0.04898962]\n",
      " [-0.13159821  0.0538227   0.04977427  0.05510829 -0.11458077 -0.11931653\n",
      "   0.05322698  0.05160214  0.05123875  0.05072238]\n",
      " [-0.11635467  0.05439451  0.05030517  0.05569451 -0.12247788 -0.13054062\n",
      "   0.05379137  0.05214696  0.05178224  0.05125842]]\n",
      "[[-0.10191948  0.0491006   0.04541024  0.05027483 -0.11469236 -0.11681522\n",
      "   0.04855712  0.0470713   0.04674291  0.04627005]\n",
      " [-0.11483566  0.05278226  0.04881381  0.05404357 -0.11769824 -0.12589124\n",
      "   0.05219697  0.05060171  0.05024746  0.04973935]\n",
      " [-0.12042886  0.0528764   0.04889988  0.0541394  -0.11427433 -0.12436095\n",
      "   0.05228983  0.05069299  0.0503371   0.04982854]\n",
      " [-0.11880283  0.05247445  0.04852927  0.05372893 -0.11863002 -0.1189082\n",
      "   0.05189428  0.05030773  0.04995527  0.0494511 ]\n",
      " [-0.11599495  0.05367087  0.049636    0.05495375 -0.12144461 -0.12702147\n",
      "   0.05307626  0.05145352  0.0510936   0.05057702]\n",
      " [-0.11726244  0.05175244  0.0478616   0.05298972 -0.11729418 -0.11688186\n",
      "   0.05118039  0.04961556  0.04926798  0.04877079]\n",
      " [-0.12140484  0.05332147  0.04931183  0.05459545 -0.11693206 -0.12375175\n",
      "   0.05273063  0.05111975  0.05076108  0.05024844]\n",
      " [-0.11983093  0.05270045  0.04873669  0.05395874 -0.1115745  -0.12646059\n",
      "   0.05211484  0.05052414  0.05016917  0.04966198]\n",
      " [-0.11337855  0.05076379  0.04694896  0.05197902 -0.12287765 -0.10847766\n",
      "   0.05020545  0.04866774  0.04832794  0.04784095]\n",
      " [-0.12570579  0.05301275  0.04902501  0.05427842 -0.11091113 -0.12337245\n",
      "   0.05242431  0.05082463  0.05046687  0.04995737]\n",
      " [-0.12848516  0.054723    0.05060858  0.05603143 -0.1237932  -0.11933603\n",
      "   0.05411904  0.05246446  0.05209644  0.05157144]\n",
      " [-0.11465681  0.05227701  0.04834803  0.05352785 -0.12400282 -0.11634521\n",
      "   0.05170053  0.05011789  0.04976786  0.04926568]\n",
      " [-0.11222223  0.05044084  0.04664886  0.05164685 -0.11512818 -0.11518063\n",
      "   0.04988315  0.0483577   0.04801924  0.04753441]\n",
      " [-0.10632065  0.0518929   0.04799406  0.05313522 -0.12765884 -0.11841663\n",
      "   0.05132079  0.04974819  0.04940207  0.04890289]\n",
      " [-0.1289193   0.0546001   0.05049426  0.05590497 -0.12047187 -0.12138515\n",
      "   0.05399643  0.05234664  0.051979    0.05145493]\n",
      " [-0.11422145  0.05216706  0.04824466  0.05341352 -0.1155362  -0.12448941\n",
      "   0.05158844  0.05001204  0.04966175  0.04915958]\n",
      " [-0.10950644  0.04985207  0.04610555  0.05104511 -0.11925735 -0.10977518\n",
      "   0.04930277  0.04779319  0.04745951  0.04698077]\n",
      " [-0.10926957  0.05110924  0.0472685   0.05233239 -0.12290486 -0.11490014\n",
      "   0.05054567  0.04899781  0.04865609  0.04816486]\n",
      " [-0.1226969   0.05460527  0.05049882  0.05590951 -0.11807426 -0.13003175\n",
      "   0.05399916  0.0523501   0.05198275  0.05145729]\n",
      " [-0.10545864  0.04924869  0.04554796  0.05042752 -0.11943626 -0.10954643\n",
      "   0.04870608  0.0472142   0.04688503  0.04641184]\n",
      " [-0.11592816  0.05266048  0.04870223  0.05392007 -0.12282332 -0.11885503\n",
      "   0.05207902  0.05048552  0.05013262  0.04962657]\n",
      " [-0.1143013   0.05163073  0.04774915  0.05286499 -0.11698681 -0.11932152\n",
      "   0.0510594   0.04949828  0.04915178  0.04865531]\n",
      " [-0.11467884  0.05334173  0.04933094  0.05461601 -0.11733591 -0.13020619\n",
      "   0.05274928  0.05113768  0.05077962  0.05026569]\n",
      " [-0.11589808  0.0519939   0.0480864   0.05323833 -0.12469249 -0.11249515\n",
      "   0.05142151  0.04984695  0.0494988   0.04899982]\n",
      " [-0.10586924  0.04886239  0.04519048  0.05003187 -0.11773105 -0.1082178\n",
      "   0.04832401  0.0468441   0.0465173   0.04604793]\n",
      " [-0.11807316  0.05230593  0.04837287  0.05355579 -0.11552239 -0.12159716\n",
      "   0.05172645  0.05014596  0.04979433  0.04929138]\n",
      " [-0.11067736  0.05143734  0.04757158  0.05266802 -0.12207888 -0.11654485\n",
      "   0.05086966  0.04931243  0.04896825  0.04847379]\n",
      " [-0.11875307  0.05330751  0.04929922  0.05458125 -0.11777445 -0.1254653\n",
      "   0.05271659  0.05110583  0.05074763  0.05023479]\n",
      " [-0.11013798  0.04980774  0.04606361  0.05099882 -0.11474865 -0.11334607\n",
      "   0.04925733  0.04775065  0.04741664  0.04693791]\n",
      " [-0.1216324   0.0539073   0.04985547  0.0551971  -0.12700278 -0.11744248\n",
      "   0.05331326  0.05168156  0.05132017  0.05080279]\n",
      " [-0.10628221  0.05071201  0.04690086  0.0519252  -0.12024561 -0.11784507\n",
      "   0.05015164  0.04861642  0.04827737  0.04778939]\n",
      " [-0.11108105  0.05229036  0.04836051  0.05354135 -0.12387451 -0.12013668\n",
      "   0.05171285  0.05012988  0.04978011  0.04927718]\n",
      " [-0.1212394   0.05352778  0.04950385  0.05480789 -0.12317983 -0.11907761\n",
      "   0.05293685  0.05131767  0.05095841  0.05044439]\n",
      " [-0.11329771  0.05267398  0.04871274  0.05393164 -0.11270158 -0.13168307\n",
      "   0.05208768  0.05049739  0.0501434   0.04963552]\n",
      " [-0.10490643  0.0502805   0.0465021   0.05148365 -0.12067087 -0.11586686\n",
      "   0.04972538  0.04820271  0.04786677  0.04738304]\n",
      " [-0.11617564  0.05110727  0.04726503  0.05232926 -0.11635913 -0.11452358\n",
      "   0.05054266  0.04899714  0.04865393  0.04816307]\n",
      " [-0.11155468  0.05161426  0.04773279  0.05284674 -0.11093497 -0.12799776\n",
      "   0.05104007  0.04948159  0.04913474  0.04863723]\n",
      " [-0.12052425  0.05179083  0.0478969   0.05302909 -0.11705303 -0.11412421\n",
      "   0.05121895  0.04965306  0.04930486  0.0488078 ]\n",
      " [-0.11465701  0.05086075  0.04703647  0.05207617 -0.11282744 -0.11789503\n",
      "   0.05029744  0.04876044  0.04841861  0.04792959]\n",
      " [-0.13446785  0.05560524  0.05142167  0.0569322  -0.11294373 -0.13018011\n",
      "   0.05498725  0.05331054  0.05293459  0.0524002 ]\n",
      " [-0.10564884  0.05072883  0.04691724  0.05194321 -0.12412548 -0.1147174\n",
      "   0.05016968  0.04863259  0.04829397  0.04780619]\n",
      " [-0.10939357  0.05093822  0.04710928  0.05215623 -0.11748488 -0.11902844\n",
      "   0.05037463  0.04883372  0.04849247  0.04800235]\n",
      " [-0.11501797  0.05227267  0.0483426   0.05352198 -0.11719347 -0.12275516\n",
      "   0.05169358  0.05011349  0.0497626   0.04925969]\n",
      " [-0.12056238  0.05235857  0.04842141  0.05360973 -0.11542006 -0.11956948\n",
      "   0.05177895  0.05019694  0.04984469  0.04934161]\n",
      " [-0.1180955   0.05227372  0.04834381  0.05352356 -0.11901349 -0.11786978\n",
      "   0.05169604  0.05011526  0.04976428  0.0492621 ]\n",
      " [-0.11817536  0.05183409  0.04793624  0.05307249 -0.11339186 -0.12042092\n",
      "   0.05125966  0.04969381  0.0493451   0.04884675]\n",
      " [-0.11583081  0.0522346   0.04830704  0.05348272 -0.11558198 -0.12329405\n",
      "   0.05165552  0.05007712  0.04972621  0.04922362]\n",
      " [-0.11751962  0.05198439  0.04807631  0.05322739 -0.11869813 -0.11679682\n",
      "   0.05141007  0.04983792  0.04948892  0.04898957]\n",
      " [-0.13159808  0.05382265  0.04977422  0.05510824 -0.11458065 -0.1193164\n",
      "   0.05322692  0.05160208  0.0512387   0.05072233]\n",
      " [-0.11635456  0.05439445  0.05030511  0.05569445 -0.12247776 -0.13054048\n",
      "   0.05379131  0.05214691  0.05178219  0.05125837]]\n",
      "W2:7.31678756455828e-08\n",
      "[-0.23327113  0.10446477  0.0966112   0.10696227 -0.23734704 -0.23877509\n",
      "  0.10330953  0.10015065  0.09944947  0.09844537]\n",
      "[-0.2332709   0.10446467  0.0966111   0.10696216 -0.23734679 -0.23877483\n",
      "  0.10330942  0.10015054  0.09944936  0.09844527]\n",
      "b2:1.466067721694886e-07\n"
     ]
    }
   ],
   "source": [
    "#误差反向传播法梯度确认\n",
    "#数值微分不易出错，反向传播容易出错。需要进行梯度确认：确认数值微分求出的梯度结果和误差反向传播算法求出的结果是否一致\n",
    "from dataset.mnist import load_mnist\n",
    "from two_layer_net import TwoLayerNet\n",
    "#读入数据\n",
    "(x_train,t_train),(x_test,t_test)=load_mnist(normalize=True,one_hot_label=True)\n",
    "network=TwoLayerNet(input_size=784,hidden_size=50,output_size=10)\n",
    "x_batch=x_train[:3]\n",
    "t_batch=t_train[:3]\n",
    "grad_numerical=network.numerical_gradient(x_batch,t_batch)\n",
    "grad_backprop=network.gradient(x_batch,t_batch)\n",
    "#print(grad_numerical.keys())\n",
    "#求各个权重的绝对误差的平均值\n",
    "for key in grad_numerical.keys():\n",
    "    print(grad_backprop[key])\n",
    "    print(grad_numerical[key])\n",
    "    diff=np.average(np.abs(grad_backprop[key]-grad_numerical[key]))\n",
    "    print(key+\":\"+str(diff))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "babafd8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0971 0.0979\n",
      "0.0971 0.0979\n",
      "0.09736666666666667 0.0982\n",
      "0.09736666666666667 0.0982\n",
      "0.78615 0.7927\n",
      "0.7861666666666667 0.7928\n",
      "0.7866166666666666 0.7936\n",
      "0.78675 0.7937\n",
      "0.8783166666666666 0.883\n",
      "0.8783333333333333 0.8829\n",
      "0.87905 0.8836\n",
      "0.8790333333333333 0.8836\n",
      "0.8986833333333333 0.9003\n",
      "0.8986 0.9004\n",
      "0.8985166666666666 0.9006\n",
      "0.8985666666666666 0.9011\n",
      "0.9069 0.9093\n",
      "0.9069166666666667 0.9093\n",
      "0.907 0.9096\n",
      "0.90695 0.9094\n",
      "0.9127666666666666 0.9151\n",
      "0.91275 0.915\n",
      "0.9130333333333334 0.9145\n",
      "0.9130333333333334 0.9145\n",
      "0.9178666666666667 0.9205\n",
      "0.9178833333333334 0.9206\n",
      "0.9178333333333333 0.9203\n",
      "0.9178333333333333 0.9201\n",
      "0.9216333333333333 0.9237\n",
      "0.9216666666666666 0.9236\n",
      "0.92175 0.9227\n",
      "0.9217166666666666 0.9227\n",
      "0.9257833333333333 0.9278\n",
      "0.9257833333333333 0.9278\n",
      "0.9259333333333334 0.9278\n",
      "0.92595 0.9278\n",
      "0.9294 0.9308\n",
      "0.9293833333333333 0.9308\n",
      "0.9294166666666667 0.9309\n",
      "0.9294166666666667 0.9309\n",
      "0.9326 0.9337\n",
      "0.9326 0.9338\n",
      "0.93265 0.9341\n",
      "0.9326333333333333 0.9343\n",
      "0.9353 0.9369\n",
      "0.9352666666666667 0.9369\n",
      "0.9353166666666667 0.9367\n",
      "0.9353 0.9366\n",
      "0.9377 0.9386\n",
      "0.9377 0.9386\n",
      "0.9378166666666666 0.9391\n",
      "0.9377833333333333 0.9391\n",
      "0.9404166666666667 0.9401\n",
      "0.9403833333333333 0.9401\n",
      "0.9404166666666667 0.9402\n",
      "0.9403666666666667 0.9402\n",
      "0.9424666666666667 0.9415\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-49-1654b1389dcb>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     22\u001b[0m         \u001b[0mtrain_loss_list\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m%\u001b[0m\u001b[0miter_per_epoch\u001b[0m\u001b[1;33m==\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 24\u001b[1;33m             \u001b[0mtrain_acc\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnetwork\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maccuracy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mt_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     25\u001b[0m             \u001b[0mtest_acc\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnetwork\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maccuracy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mt_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m             \u001b[0mtrain_acc_list\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_acc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\two_layer_net.py\u001b[0m in \u001b[0;36maccuracy\u001b[1;34m(self, x, t)\u001b[0m\n\u001b[0;32m     34\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     35\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0maccuracy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 36\u001b[1;33m         \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     37\u001b[0m         \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     38\u001b[0m         \u001b[0mt\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\two_layer_net.py\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     20\u001b[0m         \u001b[0mb1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mb2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'b1'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'b2'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 22\u001b[1;33m         \u001b[0ma1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mW1\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mb1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     23\u001b[0m         \u001b[0mz1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msigmoid\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m         \u001b[0ma2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mz1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mW2\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mb2\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#利用误差反向传播算法的学习\n",
    "(x_train,t_train),(x_test,t_test)=load_mnist(normalize=True,one_hot_label=True)\n",
    "network=TwoLayerNet(input_size=784,hidden_size=50,output_size=10)\n",
    "iters_num=10000\n",
    "train_size=x_train.shape[0]\n",
    "batch_size=100\n",
    "learning_rate=0.1\n",
    "train_loss_list=[]\n",
    "train_acc_list=[]\n",
    "test_acc_list=[]\n",
    "iter_per_epoch=max(train_size/batch_size,1)\n",
    "for i in range(iters_num):\n",
    "    batch_mask=np.random.choice(train_size,batch_size)\n",
    "    x_batch=x_train[batch_mask]\n",
    "    t_batch=t_train[batch_mask]\n",
    "    #通过误差反向传播法求梯度\n",
    "    grad=network.gradient(x_batch,t_batch)\n",
    "    #更新\n",
    "    for key in (\"W1\",\"b1\",\"W2\",\"b2\"):\n",
    "        network.params[key]-=learning_rate*grad[key]\n",
    "        loss=network.loss(x_batch,t_batch)\n",
    "        train_loss_list.append(loss)\n",
    "        if i%iter_per_epoch==0:\n",
    "            train_acc=network.accuracy(x_train,t_train)\n",
    "            test_acc=network.accuracy(x_test,t_test)\n",
    "            train_acc_list.append(train_acc)\n",
    "            test_acc_list.append(test_acc)\n",
    "            print(train_acc,test_acc)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
